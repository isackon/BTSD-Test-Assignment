{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom PIL import Image\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:04:54.536501Z","iopub.execute_input":"2022-03-11T16:04:54.536884Z","iopub.status.idle":"2022-03-11T16:04:54.544982Z","shell.execute_reply.started":"2022-03-11T16:04:54.536846Z","shell.execute_reply":"2022-03-11T16:04:54.543643Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"# Hyperparameters etc.\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 1\nNUM_EPOCHS = 15\nNUM_WORKERS = 2\nIMAGE_HEIGHT = 160  # 576 originally\nIMAGE_WIDTH = 240  # 768 originally\nPIN_MEMORY = True\nLOAD_MODEL = True\nTRAIN_PERCENT = 0.7\nVAL_PERCENT = 0.15\nIMG_LEN = 240\n\nIMG_DIR = \"../input/btsd-test/imgs\"\n\nFILE_LIST = []\n\nfor root, dirs, files in os.walk(IMG_DIR):\n    for filename in files:\n        FILE_LIST.append(filename)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:04:54.946180Z","iopub.execute_input":"2022-03-11T16:04:54.946839Z","iopub.status.idle":"2022-03-11T16:04:54.960175Z","shell.execute_reply.started":"2022-03-11T16:04:54.946775Z","shell.execute_reply":"2022-03-11T16:04:54.959126Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNET(nn.Module):\n    def __init__(\n            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n    ):\n        super(UNET, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2,\n                )\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n\n        return self.final_conv(x)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:04:55.613468Z","iopub.execute_input":"2022-03-11T16:04:55.614498Z","iopub.status.idle":"2022-03-11T16:04:55.634050Z","shell.execute_reply.started":"2022-03-11T16:04:55.614462Z","shell.execute_reply":"2022-03-11T16:04:55.632217Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class BTSDDataset(Dataset):\n    def __init__(self, image_dir, file_list, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.file_list = file_list\n        self.images = os.listdir(image_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        \n        img_path = os.path.join(self.image_dir, self.file_list[index])\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n\n        if self.transform is not None:\n            augmentations = self.transform(image=image)\n            image = augmentations[\"image\"]\n\n        return image, self.file_list[index]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:04:57.140447Z","iopub.execute_input":"2022-03-11T16:04:57.140727Z","iopub.status.idle":"2022-03-11T16:04:57.150854Z","shell.execute_reply.started":"2022-03-11T16:04:57.140695Z","shell.execute_reply":"2022-03-11T16:04:57.149500Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\ndef get_loaders(\n    img_dir,\n    test_file_list,\n    batch_size,\n    test_transform,\n    num_workers=4,\n    pin_memory=True,\n):\n    test_ds = BTSDDataset(\n        image_dir=img_dir,\n        file_list=test_file_list,\n        transform=test_transform,\n    )\n\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=False,\n    )\n\n    return test_loader\n\n\ndef save_predictions_as_imgs(\n    loader, model, folder=\"./saved_images\", device=\"cuda\"\n):\n    print(enumerate(loader))\n    model.eval()\n    for idx, (x, name) in enumerate(loader):\n        print(name[0].split('.')[0])\n        x = x.to(device=device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n        torchvision.utils.save_image(\n            preds, f\"{name[0].split('.')[0]}.png\"\n        )","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:05:31.074412Z","iopub.execute_input":"2022-03-11T16:05:31.074721Z","iopub.status.idle":"2022-03-11T16:05:31.087630Z","shell.execute_reply.started":"2022-03-11T16:05:31.074686Z","shell.execute_reply":"2022-03-11T16:05:31.086403Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"def main():\n\n    test_transforms = A.Compose(\n        [\n            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n\n    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    test_loader = get_loaders(\n        IMG_DIR,\n        FILE_LIST,\n        BATCH_SIZE,\n        test_transforms,\n        NUM_WORKERS,\n        PIN_MEMORY,\n    )\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(\"../input/btsd-assignment-models/3_v1.0_Baseline_10epochs_160x240.pth.tar\"), model)\n\n    # print some examples to a folder\n    save_predictions_as_imgs(\n        test_loader, model, folder=\"./saved_images\", device=DEVICE\n    )\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:05:31.960387Z","iopub.execute_input":"2022-03-11T16:05:31.960753Z","iopub.status.idle":"2022-03-11T16:05:32.909770Z","shell.execute_reply.started":"2022-03-11T16:05:31.960705Z","shell.execute_reply":"2022-03-11T16:05:32.908313Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}